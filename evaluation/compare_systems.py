#!/usr/bin/env python3
"""
GraphRAG vs Naive RAG Comparison System
======================================

Compares GraphRAG and Naive RAG performance using GPT-5 ground truth
to demonstrate the advantages of graph-based retrieval for structured queries.
"""

from dotenv import load_dotenv
load_dotenv(override=True)

import json
import time
import asyncio
from pathlib import Path
from typing import List, Dict, Any
import logging
import os
import httpx
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from ragas import evaluate, EvaluationDataset
from ragas.metrics import ContextPrecision, Faithfulness, AnswerRelevancy, ContextRecall, FactualCorrectness
from ragas.dataset_schema import SingleTurnSample
import asyncio

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SystemComparator:
    """Compares GraphRAG, Naive RAG, and Ground Truth systems."""

    def __init__(self):
        """Initialize the comparator."""

        self.results_dir = Path(os.getenv("TEST_DIR", "data/evaluation"))
        self.results_dir.mkdir(exist_ok=True)

        # Backend URL (env overrides config)
        self.backend_url = os.getenv(
            "BACKEND_URL",
            "http://localhost:8000"
        ).rstrip("/")

        # Systems will be initialized when needed
        self.graph_rag_system = None
        self.naive_rag_system = None

        self.api_key = os.environ.get("OPENAI_API_KEY")

        if not self.api_key:
            raise ValueError("Missing OPENAI_API_KEY!")
        
        # 3. Inicjalizacja modeli dla Ragasa
        logger.info("Initializing OpenAI models for evaluation...")
        
        self.eval_llm = ChatOpenAI(
            model="gpt-4o",  # or gpt-4-turbo
            temperature=0,
            api_key=self.api_key
        )
        
        self.eval_embeddings = OpenAIEmbeddings(
            model="text-embedding-3-small",
            api_key=self.api_key
        )

        logger.info(f"‚úì System Comparator initialized (backend={self.backend_url})")
    

    def load_ground_truth(self) -> Dict[str, Any]:
        """Load ground truth answers generated by GPT-5."""
        ground_truth_file = self.results_dir / "ground_truth_answers.json"

        logger.info(f"‚úì Using existing ground truth: {ground_truth_file}")

        with open(ground_truth_file, 'r') as f:
            data = json.load(f)

        logger.info(f"‚úì Loaded ground truth with {len(data['ground_truth_answers'])} questions")
        return data

    def run_graph_rag_query(self, question: str) -> Dict[str, Any]:
        """Run a query through the GraphRAG backend (/ask_graph)."""
        try:
            start_time = time.time()
            url = f"{self.backend_url}/ask_graph"
            resp = httpx.post(url, json={"question": question}, timeout=3000.0)
            resp.raise_for_status()
            execution_time = time.time() - start_time

            data = resp.json()
            return {
                "question": question,
                "answer": data.get("answer", "No answer"),
                "context_documents": data.get("context_documents", []),
                "cypher_query": data.get("cypher_query", ""),
                "execution_time": execution_time,
                "success": True,
                "system": "graphrag",
                "raw": data
            }

        except Exception as e:
            logger.error(f"GraphRAG query failed for '{question}': {e}")
            return {
                "question": question,
                "answer": f"Error: {str(e)}",
                "context_documents": [],
                "cypher_query": "",
                "execution_time": 0,
                "success": False,
                "system": "graphrag",
                "error": str(e)
            }

    def run_naive_rag_query(self, question: str) -> Dict[str, Any]:
        """Run a query through the Naive RAG backend (/ask_rag)."""
        try:
            start_time = time.time()
            url = f"{self.backend_url}/ask_rag"
            top_k = int(os.getenv("TOP_K", "5"))
            resp = httpx.post(url, json={"question": question, "top_k": top_k}, timeout=3000.0)
            resp.raise_for_status()
            execution_time = time.time() - start_time

            data = resp.json()
            return {
                "question": question,
                "answer": data.get("answer", "No answer"),
                "context_documents": data.get("context_documents", []),
                "execution_time": execution_time,
                "num_chunks_retrieved": len(data.get("context_documents", [])),
                "success": True,
                "system": "naive_rag",
                "raw": data
            }

        except Exception as e:
            logger.error(f"Naive RAG query failed for '{question}': {e}")
            return {
                "question": question,
                "answer": f"Error: {str(e)}",
                "context_documents": [],
                "execution_time": 0,
                "num_chunks_retrieved": 0,
                "success": False,
                "system": "naive_rag",
                "error": str(e)
            }

    async def run_full_comparison(self) -> Dict[str, Any]:
        """Run complete comparison utilizing Ragas for evaluation."""
        
        # Data preparation
        ground_truth_data = self.load_ground_truth()
        questions = ground_truth_data["ground_truth_answers"]

        samples_graph = []
        samples_naive = []
        # Container for operational metadata (execution time, success, cypher query etc.)
        operational_data = []

        logger.info(f"Starting generation for {len(questions)} questions...")

        # Generating answers
        for i, item in enumerate(questions):
            q = item["question"]
            gt = item["ground_truth_answer"]
            
            logger.info(f"[{i+1}/{len(questions)}] Generating: {q[:40]}...")

            graph_res = self.run_graph_rag_query(q)
            # Convert context to strings
            graph_context_str = [str(c) for c in graph_res.get("context_documents", [])]
            print("GraphRAG retrieved contexts:")
            print(graph_context_str)
            
            samples_graph.append(SingleTurnSample(
                user_input=q,
                response=graph_res["answer"],
                retrieved_contexts=graph_context_str,
                reference=gt
            ))

            naive_res = self.run_naive_rag_query(q)
            naive_context_str = [str(c) for c in naive_res.get("context_documents", [])]
            print("Naive RAG retrieved contexts:")
            print(naive_context_str)

            samples_naive.append(SingleTurnSample(
                user_input=q,
                response=naive_res["answer"],
                retrieved_contexts=naive_context_str,
                reference=gt
            ))

            operational_data.append({
                "question_index": i + 1,
                "question": q,
                "category": item["category"],
                "ground_truth": gt,
                "graphrag_meta": {
                    "execution_time": graph_res["execution_time"],
                    "cypher_query": graph_res.get("cypher_query", ""),
                    "success": graph_res["success"]
                },
                "naiverag_meta": {
                    "execution_time": naive_res["execution_time"],
                    "chunks_count": naive_res.get("num_chunks_retrieved", 0),
                    "success": naive_res["success"]
                }
            })
            
            # Pause for rate limits
            await asyncio.sleep(0.5)

        # Evaluation
        logger.info("Running Ragas evaluation (this may take a while)...")
        
        metrics = [ContextPrecision(), Faithfulness(), AnswerRelevancy(), ContextRecall(), FactualCorrectness()]

        logger.info("Evaluating GraphRAG results...")
        graph_scores = evaluate(
            dataset=EvaluationDataset(samples_graph), 
            metrics=metrics,
            llm=self.eval_llm,
            embeddings=self.eval_embeddings
        )
        
        logger.info("Evaluating Naive RAG results...")
        naive_scores = evaluate(
            dataset=EvaluationDataset(samples_naive), 
            metrics=metrics,
            llm=self.eval_llm,
            embeddings=self.eval_embeddings
        )

        # Merging results
        graph_scores_list = graph_scores.to_pandas().to_dict(orient="records")
        naive_scores_list = naive_scores.to_pandas().to_dict(orient="records")

        final_results = []
        
        for i, op_data in enumerate(operational_data):
            
            # WyciƒÖgamy wyniki dla konkretnego pytania
            # Use the pandas-converted lists (indexable by integer)
            g_score = graph_scores_list[i]
            n_score = naive_scores_list[i]

            comparison_entry = {
                **op_data,
                "graphrag": {
                    "answer": samples_graph[i].response,
                    **op_data["graphrag_meta"],
                    "evaluation": g_score
                },
                "naive_rag": {
                    "answer": samples_naive[i].response,
                    **op_data["naiverag_meta"],
                    "evaluation": n_score
                }
            }
            # Deleting temporary meta keys, to avoid duplication
            del comparison_entry["graphrag_meta"]
            del comparison_entry["naiverag_meta"]
            
            final_results.append(comparison_entry)

        # Build a proper summary using `generate_summary` and include ragas numeric averages
        ragas_avg = {
            "graphrag_avg": graph_scores.to_pandas().mean(numeric_only=True).to_dict(),
            "naive_avg": naive_scores.to_pandas().mean(numeric_only=True).to_dict()
        }

        summary = self.generate_summary(final_results)
        summary["ragas_averages"] = ragas_avg

        return {
            "metadata": {
                "total_questions": len(questions),
                "evaluator": "Ragas + OpenAI",
                "systems": ["GraphRAG", "Naive RAG"]
            },
            "results": final_results,
            "summary": summary
        }

    def _calculate_score(self, evaluation_metrics: Dict[str, float]) -> float:
        """Helper: Calculate average score from Ragas metrics."""
        if not evaluation_metrics:
            return 0.0
        # Average from available metrics (faithfulness, answer_relevancy, etc.)
        values = [v for v in evaluation_metrics.values() if isinstance(v, (int, float))]
        return sum(values) / len(values) if values else 0.0

    def generate_summary(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Generate summary statistics for the comparison (Adapted for Ragas)."""
        graph_wins = 0
        naive_wins = 0
        ties = 0

        graph_quality_scores = []
        naive_quality_scores = []

        category_stats = {}

        for result in results:
            category = result["category"]
            
            graph_metrics = result["graphrag"]["evaluation"]
            naive_metrics = result["naive_rag"]["evaluation"]
            
            graph_score = self._calculate_score(graph_metrics)
            naive_score = self._calculate_score(naive_metrics)

            graph_quality_scores.append(graph_score)
            naive_quality_scores.append(naive_score)

            # Track category performance
            if category not in category_stats:
                category_stats[category] = {
                    "total": 0,
                    "graph_wins": 0,
                    "naive_wins": 0,
                    "ties": 0
                }

            category_stats[category]["total"] += 1

            # Determine winner (with small tolerance for floats)
            if graph_score > naive_score + 0.01:
                graph_wins += 1
                category_stats[category]["graph_wins"] += 1
            elif naive_score > graph_score + 0.01:
                naive_wins += 1
                category_stats[category]["naive_wins"] += 1
            else:
                ties += 1
                category_stats[category]["ties"] += 1

        import statistics

        summary = {
            "overall_performance": {
                "graphrag_wins": graph_wins,
                "naive_rag_wins": naive_wins,
                "ties": ties,
                "graphrag_win_rate": graph_wins / len(results) if results else 0,
                "naive_rag_win_rate": naive_wins / len(results) if results else 0
            },
            "quality_scores": {
                "graphrag_avg": statistics.mean(graph_quality_scores) if graph_quality_scores else 0,
                "naive_rag_avg": statistics.mean(naive_quality_scores) if naive_quality_scores else 0,
                "graphrag_median": statistics.median(graph_quality_scores) if graph_quality_scores else 0,
                "naive_rag_median": statistics.median(naive_quality_scores) if naive_quality_scores else 0
            },
            "category_breakdown": category_stats
        }

        return summary

    def save_comparison_results(self, comparison_data: Dict[str, Any]) -> Path:
        """Save comparison results to file."""
        output_file = self.results_dir / "system_comparison_results.json"

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(comparison_data, f, indent=2, ensure_ascii=False)

        logger.info(f"‚úì Comparison results saved to: {output_file}")
        return output_file

    def generate_comparison_table(self, comparison_data: Dict[str, Any]) -> str:
        """Generate a readable comparison table."""
        results = comparison_data["results"]
        summary = comparison_data["summary"]

        table_lines = []
        table_lines.append("# GraphRAG vs Naive RAG Comparison Results")
        table_lines.append("")
        table_lines.append("## Summary")
        table_lines.append(f"- **GraphRAG Wins**: {summary['overall_performance']['graphrag_wins']}")
        table_lines.append(f"- **Naive RAG Wins**: {summary['overall_performance']['naive_rag_wins']}")
        table_lines.append(f"- **Ties**: {summary['overall_performance']['ties']}")
        table_lines.append(f"- **GraphRAG Win Rate**: {summary['overall_performance']['graphrag_win_rate']:.1%}")
        table_lines.append("")
        table_lines.append(f"- **GraphRAG Avg Quality**: {summary['quality_scores']['graphrag_avg']:.2f}")
        table_lines.append(f"- **Naive RAG Avg Quality**: {summary['quality_scores']['naive_rag_avg']:.2f}")
        table_lines.append("")

        # Category breakdown
        table_lines.append("## Performance by Category")
        table_lines.append("")
        for category, stats in summary["category_breakdown"].items():
            total = stats["total"]
            if total > 0:
                win_rate = stats["graph_wins"] / total
                table_lines.append(f"### {category.title()}")
                table_lines.append(f"- GraphRAG: {stats['graph_wins']}/{total} ({win_rate:.1%})")
                table_lines.append("")

        # Detailed results table
        table_lines.append("## Detailed Results")
        table_lines.append("")
        # Updated header to show scores
        table_lines.append("| # | Question | Graph Answer | Naive Answer | Graph Score | Naive Score | Winner |")
        table_lines.append("|---|----------|--------------|--------------|-------------|-------------|--------|")

        for result in results:
            idx = result["question_index"]
            question = result["question"][:40] + "..." if len(result["question"]) > 40 else result["question"]
            
            graph_answer = result["graphrag"]["answer"][:30] + "..." if len(result["graphrag"]["answer"]) > 30 else result["graphrag"]["answer"]
            naive_answer = result["naive_rag"]["answer"][:30] + "..." if len(result["naive_rag"]["answer"]) > 30 else result["naive_rag"]["answer"]

            graph_score = self._calculate_score(result["graphrag"]["evaluation"])
            naive_score = self._calculate_score(result["naive_rag"]["evaluation"])

            if graph_score > naive_score + 0.01:
                winner = "GraphRAG"
            elif naive_score > graph_score + 0.01:
                winner = "Naive RAG"
            else:
                winner = "Tie"

            # Escape pipes
            question = question.replace("|", "\\|").replace("\n", " ")
            graph_answer = graph_answer.replace("|", "\\|").replace("\n", " ")
            naive_answer = naive_answer.replace("|", "\\|").replace("\n", " ")

            table_lines.append(f"| {idx} | {question} | {graph_answer} | {naive_answer} | {graph_score:.2f} | {naive_score:.2f} | {winner} |")

        return "\n".join(table_lines)
    
    def display_results(self, comparison_data: Dict[str, Any]) -> None:
        """Display comparison results to stdout."""
        summary = comparison_data["summary"]
        perf = summary["overall_performance"]
        scores = summary["quality_scores"]

        print("\n" + "="*80)
        print("GRAPHRAG vs NAIVE RAG COMPARISON RESULTS")
        print("="*80)

        print(f"\nOVERALL WINNER:")
        if perf["graphrag_wins"] > perf["naive_rag_wins"]:
            print(f"   GraphRAG wins! ({perf['graphrag_wins']} vs {perf['naive_rag_wins']})")
        elif perf["naive_rag_wins"] > perf["graphrag_wins"]:
            print(f"   Naive RAG wins! ({perf['naive_rag_wins']} vs {perf['graphrag_wins']})")
        else:
            print(f"   It's a tie! ({perf['graphrag_wins']} vs {perf['naive_rag_wins']})")

        print(f"\nWIN RATES:")
        print(f"   ‚Ä¢ GraphRAG Win Rate: {perf['graphrag_win_rate']:.1%}")
        print(f"   ‚Ä¢ Naive RAG Win Rate: {perf['naive_rag_win_rate']:.1%}")
        print(f"   ‚Ä¢ Ties: {perf['ties']}")

        print(f"\nAVERAGE QUALITY SCORES:")
        print(f"   ‚Ä¢ GraphRAG:  {scores['graphrag_avg']:.4f}")
        print(f"   ‚Ä¢ Naive RAG: {scores['naive_rag_avg']:.4f}")

        print(f"\nBREAKDOWN BY CATEGORY:")
        for category, stats in summary["category_breakdown"].items():
            total = stats["total"]
            if total > 0:
                g_rate = stats["graph_wins"] / total
                n_rate = stats["naive_wins"] / total
                print(f"   ‚Ä¢ {category.title():<15}: GraphRAG {stats['graph_wins']} wins ({g_rate:.0%}) | Naive {stats['naive_wins']} wins ({n_rate:.0%})")
        
        print("\n" + "="*80)


async def wait_for_healthcheck(
    url: str,
    retries: int = 50,
    delay: float = 2.0,
    timeout: float = 5.0,
) -> bool:
    """
    Sprawdza endpoint /healthcheck a≈º zwr√≥ci status == 'ok'
    lub wyczerpie liczbƒô pr√≥b.

    :param url: np. "http://localhost:8000/healthcheck"
    :param retries: liczba pr√≥b
    :param delay: czas oczekiwania miƒôdzy pr√≥bami (sekundy)
    :param timeout: timeout HTTP
    :return: True je≈õli healthcheck OK, inaczej False
    """

    async with httpx.AsyncClient(timeout=timeout) as client:
        for attempt in range(1, retries + 1):
            try:
                response = await client.get(url)

                if response.status_code == 200:
                    data = response.json()

                    if data.get("status") == "ok":
                        logger.info("Healthcheck OK")
                        return True

                logger.warning(
                    "Healthcheck failed (attempt %d/%d)",
                    attempt,
                    retries,
                )

            except httpx.RequestError as exc:
                logger.warning(
                    "Healthcheck request warning (attempt %d/%d): %s",
                    attempt,
                    retries,
                    exc,
                )

            if attempt < retries:
                await asyncio.sleep(delay)

    logger.error("Healthcheck failed after %d attempts", retries)
    return False

async def main():
    """Main comparison function with enhanced output."""
    print("üöÄ GraphRAG vs Naive RAG Complete Comparison Workflow")
    print("=" * 60)


    is_up = await wait_for_healthcheck(url=os.getenv("BACKEND_URL") + "/healthcheck")

    if not is_up:
        raise RuntimeError("API nie wystartowa≈Ço")

    try:
        # Initialize comparator first to load config
        comparator = SystemComparator()

        # Run full comparison
        print("\nStarting comprehensive comparison...")
        comparison_data = await comparator.run_full_comparison()

        # Save results
        output_file = comparator.save_comparison_results(comparison_data)

        # Generate markdown table
        markdown_table = comparator.generate_comparison_table(comparison_data)

        table_file = Path(os.getenv('TEST_DIR', 'data/evaluation')) / "comparison_table.md"
        with open(table_file, 'w') as f:
            f.write(markdown_table)

        # Display results
        comparator.display_results(comparison_data)

        # Final results summary
        print("\n" + "="*60)
        print("COMPARISON WORKFLOW COMPLETED SUCCESSFULLY!")
        print("="*60)
        print("\nGenerated Files:")
        results_dir = Path(os.getenv('TEST_DIR', 'data/evaluation'))
        if results_dir.exists():
            for file in sorted(results_dir.glob("*")):
                print(f"  ‚Ä¢ {file}")

        print("\nKey Results:")
        print(f"  ‚Ä¢ Ground Truth: {results_dir}/ground_truth_answers.json")
        print(f"  ‚Ä¢ Comparison Data: {output_file}")
        print(f"  ‚Ä¢ Comparison Table: {table_file}")

        print("\nNext Steps:")
        print(f"  1. Review the comparison table: cat {table_file}")
        print(f"  2. Analyze detailed results: cat {output_file}")

        return True

    except Exception as e:
        logger.exception("Comparison failed")
        print("\nüí• Workflow failed: see logs (traceback printed to logger)")
        print("Check the logs above for traceback/details.")
        return False


if __name__ == "__main__":
    asyncio.run(main())