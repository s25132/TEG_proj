#!/usr/bin/env python3
"""
GraphRAG vs Naive RAG Comparison System
======================================

Compares GraphRAG and Naive RAG performance using GPT-5 ground truth
to demonstrate the advantages of graph-based retrieval for structured queries.
"""

from dotenv import load_dotenv
load_dotenv(override=True)

import json
import time
import asyncio
import importlib.util
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
import logging
import toml
import os
import sys
import subprocess
import httpx

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SystemComparator:
    """Compares GraphRAG, Naive RAG, and Ground Truth systems."""

    def __init__(self, config_path: str = "utils/config.toml"):
        """Initialize the comparator."""
        self.config = self._load_config(config_path)
        self.results_dir = Path("results")
        self.results_dir.mkdir(exist_ok=True)

        # Backend URL (env overrides config)
        self.backend_url = os.getenv(
            "BACKEND_URL",
            self.config.get("backend", {}).get("url", "http://localhost:8000")
        ).rstrip("/")

        # Systems will be initialized when needed
        self.graph_rag_system = None
        self.naive_rag_system = None

        logger.info(f"‚úì System Comparator initialized (backend={self.backend_url})")

    def _find_script(self, names: List[str]) -> Optional[Path]:
        """Search repository for scripts by filename (returns first match)."""
        repo_root = Path(__file__).resolve().parents[2]
        # try direct lookups first
        for name in names:
            p = repo_root / name
            if p.exists():
                logger.info(f"Found script by direct path: {p}")
                return p

        # fallback: search the whole repo
        for name in names:
            matches = list(repo_root.rglob(name))
            if matches:
                logger.info(f"Found script by rglob: {matches[0]}")
                return matches[0]

        # last resort: search cwd
        for name in names:
            matches = list(Path.cwd().rglob(name))
            if matches:
                logger.info(f"Found script in cwd by rglob: {matches[0]}")
                return matches[0]

        logger.warning(f"Could not find any of {names}")
        return None
    
    def _load_config(self, config_path: str) -> dict:
        """Load configuration from TOML file."""
        if not os.path.exists(config_path):
            raise ValueError(f"Configuration file not found: {config_path}")

        with open(config_path, 'r') as f:
            config = toml.load(f)

        return config

    def load_ground_truth(self) -> Dict[str, Any]:
        """Load ground truth answers generated by GPT-5."""
        ground_truth_file = self.results_dir / "ground_truth_answers.json"

        if not ground_truth_file.exists():
            logger.info(f"Ground truth file not found: {ground_truth_file}")
            logger.info("Generating ground truth automatically...")

            script = self._find_script(["utils/generate_ground_truth.py", "generate_ground_truth.py"])
            if script is None:
                raise FileNotFoundError("Could not locate generate_ground_truth.py in repository")

            result = subprocess.run(
                [sys.executable, str(script)],
                cwd=script.parent,
                capture_output=True,
                text=True
            )

            if result.returncode != 0:
                raise RuntimeError(
                    f"Failed to generate ground truth:\n"
                    f"STDOUT: {result.stdout}\n"
                    f"STDERR: {result.stderr}"
                )

            if not ground_truth_file.exists():
                raise FileNotFoundError(
                    f"Ground truth generation completed but file still not found: {ground_truth_file}"
                )

            logger.info("‚úì Ground truth generated successfully")
        else:
            logger.info(f"‚úì Using existing ground truth: {ground_truth_file}")

        with open(ground_truth_file, 'r') as f:
            data = json.load(f)

        logger.info(f"‚úì Loaded ground truth with {len(data['ground_truth_answers'])} questions")
        return data

    def initialize_graph_rag_system(self):
        """Check backend /ask_graph endpoint is available."""
        url = f"{self.backend_url}/ask_graph"
        try:
            resp = httpx.post(url, json={"question": "Health check"}, timeout=10.0)
            resp.raise_for_status()
            self.graph_rag_system = True
            logger.info("‚úì GraphRAG backend endpoint available")
            return True
        except Exception as e:
            logger.error(f"GraphRAG backend endpoint not reachable ({url}): {e}")
            return False

    def initialize_naive_rag_system(self):
        """Check backend /ask_rag endpoint is available."""
        url = f"{self.backend_url}/ask_rag"
        try:
            resp = httpx.post(url, json={"question": "Health check", "top_k": 1}, timeout=10.0)
            resp.raise_for_status()
            self.naive_rag_system = True
            logger.info("‚úì Naive RAG backend endpoint available")
            return True
        except Exception as e:
            logger.error(f"Naive RAG backend endpoint not reachable ({url}): {e}")
            return False

        except Exception as e:
            logger.error(f"Failed to initialize Naive RAG system: {e}")
            return False

    def run_graph_rag_query(self, question: str) -> Dict[str, Any]:
        """Run a query through the GraphRAG backend (/ask_graph)."""
        try:
            start_time = time.time()
            url = f"{self.backend_url}/ask_graph"
            resp = httpx.post(url, json={"question": question}, timeout=30.0)
            resp.raise_for_status()
            execution_time = time.time() - start_time

            data = resp.json()
            return {
                "question": question,
                "answer": data.get("answer", "No answer"),
                "cypher_query": data.get("cypher_query", ""),
                "execution_time": execution_time,
                "success": True,
                "system": "graphrag",
                "raw": data
            }

        except Exception as e:
            logger.error(f"GraphRAG query failed for '{question}': {e}")
            return {
                "question": question,
                "answer": f"Error: {str(e)}",
                "cypher_query": "",
                "execution_time": 0,
                "success": False,
                "system": "graphrag",
                "error": str(e)
            }

    def run_naive_rag_query(self, question: str) -> Dict[str, Any]:
        """Run a query through the Naive RAG backend (/ask_rag)."""
        try:
            start_time = time.time()
            url = f"{self.backend_url}/ask_rag"
            top_k = int(self.config.get("evaluation", {}).get("top_k", 5))
            resp = httpx.post(url, json={"question": question, "top_k": top_k}, timeout=30.0)
            resp.raise_for_status()
            execution_time = time.time() - start_time

            data = resp.json()
            return {
                "question": question,
                "answer": data.get("answer", "No answer"),
                "execution_time": execution_time,
                "num_chunks_retrieved": len(data.get("context_documents", [])),
                "success": True,
                "system": "naive_rag",
                "raw": data
            }

        except Exception as e:
            logger.error(f"Naive RAG query failed for '{question}': {e}")
            return {
                "question": question,
                "answer": f"Error: {str(e)}",
                "execution_time": 0,
                "num_chunks_retrieved": 0,
                "success": False,
                "system": "naive_rag",
                "error": str(e)
            }

    def evaluate_answer_quality(self, ground_truth: str, system_answer: str, question_type: str) -> Dict[str, Any]:
        """Evaluate how well a system answer matches the ground truth."""

        # Simple evaluation criteria
        evaluation = {
            "exact_match": ground_truth.strip().lower() == system_answer.strip().lower(),
            "contains_key_info": False,
            "numerical_accuracy": None,
            "completeness_score": 0.0,  # 0-1 scale
            "quality_score": 0.0  # 0-1 scale
        }

        # Extract key information based on question type
        if question_type == "counting":
            # Look for numbers in both answers
            import re
            gt_numbers = re.findall(r'\d+', ground_truth)
            sys_numbers = re.findall(r'\d+', system_answer)

            if gt_numbers and sys_numbers:
                try:
                    gt_num = int(gt_numbers[0])
                    sys_num = int(sys_numbers[0])
                    evaluation["numerical_accuracy"] = abs(gt_num - sys_num) == 0
                    evaluation["contains_key_info"] = True
                    evaluation["quality_score"] = 1.0 if gt_num == sys_num else 0.0
                except ValueError:
                    pass

        elif question_type in ["filtering", "listing"]:
            # Check if system answer contains key entities from ground truth
            gt_words = set(ground_truth.lower().split())
            sys_words = set(system_answer.lower().split())

            # Look for name-like words (capitalized in original)
            gt_names = set(word for word in ground_truth.split() if word and word[0].isupper())
            sys_names = set(word for word in system_answer.split() if word and word[0].isupper())

            if gt_names:
                name_overlap = len(gt_names.intersection(sys_names)) / len(gt_names)
                evaluation["completeness_score"] = name_overlap
                evaluation["quality_score"] = name_overlap
                evaluation["contains_key_info"] = name_overlap > 0

        elif question_type == "aggregation":
            # Similar to counting for numerical results
            import re
            gt_nums = re.findall(r'\d+\.?\d*', ground_truth)
            sys_nums = re.findall(r'\d+\.?\d*', system_answer)

            if gt_nums and sys_nums:
                try:
                    gt_val = float(gt_nums[0])
                    sys_val = float(sys_nums[0])
                    # Allow small differences for averages
                    diff = abs(gt_val - sys_val) / max(gt_val, 1)
                    evaluation["numerical_accuracy"] = diff < 0.1  # 10% tolerance
                    evaluation["quality_score"] = max(0, 1.0 - diff)
                    evaluation["contains_key_info"] = True
                except ValueError:
                    pass

        else:
            # General text comparison
            gt_words = set(ground_truth.lower().split())
            sys_words = set(system_answer.lower().split())

            if gt_words:
                word_overlap = len(gt_words.intersection(sys_words)) / len(gt_words)
                evaluation["completeness_score"] = word_overlap
                evaluation["quality_score"] = word_overlap
                evaluation["contains_key_info"] = word_overlap > 0.3

        return evaluation

    async def run_full_comparison(self) -> Dict[str, Any]:
        """Run complete comparison between all systems."""
        # Load ground truth
        ground_truth_data = self.load_ground_truth()
        questions = ground_truth_data["ground_truth_answers"]

        # Initialize systems
        if not self.initialize_graph_rag_system():
            raise Exception("Failed to initialize GraphRAG system")

        if not self.initialize_naive_rag_system():
            raise Exception("Failed to initialize Naive RAG system")

        logger.info(f"Starting comparison with {len(questions)} questions...")

        # Run all comparisons
        results = []
        for i, ground_truth_item in enumerate(questions):
            question = ground_truth_item["question"]
            category = ground_truth_item["category"]
            ground_truth_answer = ground_truth_item["ground_truth_answer"]

            logger.info(f"\n[{i+1}/{len(questions)}] Processing: {question[:50]}...")

            # Run GraphRAG query
            logger.info("  Running GraphRAG...")
            graph_result = self.run_graph_rag_query(question)

            # Run Naive RAG query
            logger.info("  Running Naive RAG...")
            naive_result = self.run_naive_rag_query(question)

            # Evaluate both answers
            graph_evaluation = self.evaluate_answer_quality(
                ground_truth_answer, graph_result["answer"], category
            )
            naive_evaluation = self.evaluate_answer_quality(
                ground_truth_answer, naive_result["answer"], category
            )

            # Compile comparison
            comparison = {
                "question_index": i + 1,
                "question": question,
                "category": category,
                "ground_truth": ground_truth_answer,

                "graphrag": {
                    "answer": graph_result["answer"],
                    "cypher_query": graph_result.get("cypher_query", ""),
                    "execution_time": graph_result["execution_time"],
                    "success": graph_result["success"],
                    "evaluation": graph_evaluation
                },

                "naive_rag": {
                    "answer": naive_result["answer"],
                    "chunks_retrieved": naive_result.get("num_chunks_retrieved", 0),
                    "execution_time": naive_result["execution_time"],
                    "success": naive_result["success"],
                    "evaluation": naive_evaluation
                }
            }

            results.append(comparison)

            # Small delay to be nice to APIs
            await asyncio.sleep(0.5)

        # Compile final comparison data
        comparison_data = {
            "metadata": {
                "comparison_date": datetime.now().isoformat(),
                "total_questions": len(results),
                "ground_truth_source": "GPT-5",
                "systems_compared": ["GraphRAG", "Naive RAG"]
            },
            "results": results,
            "summary": self.generate_summary(results)
        }

        return comparison_data

    def generate_summary(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Generate summary statistics for the comparison."""
        graph_wins = 0
        naive_wins = 0
        ties = 0

        graph_quality_scores = []
        naive_quality_scores = []

        category_stats = {}

        for result in results:
            category = result["category"]
            graph_score = result["graphrag"]["evaluation"]["quality_score"]
            naive_score = result["naive_rag"]["evaluation"]["quality_score"]

            graph_quality_scores.append(graph_score)
            naive_quality_scores.append(naive_score)

            # Track category performance
            if category not in category_stats:
                category_stats[category] = {
                    "total": 0,
                    "graph_wins": 0,
                    "naive_wins": 0,
                    "ties": 0
                }

            category_stats[category]["total"] += 1

            # Determine winner
            if graph_score > naive_score:
                graph_wins += 1
                category_stats[category]["graph_wins"] += 1
            elif naive_score > graph_score:
                naive_wins += 1
                category_stats[category]["naive_wins"] += 1
            else:
                ties += 1
                category_stats[category]["ties"] += 1

        import statistics

        summary = {
            "overall_performance": {
                "graphrag_wins": graph_wins,
                "naive_rag_wins": naive_wins,
                "ties": ties,
                "graphrag_win_rate": graph_wins / len(results),
                "naive_rag_win_rate": naive_wins / len(results)
            },
            "quality_scores": {
                "graphrag_avg": statistics.mean(graph_quality_scores),
                "naive_rag_avg": statistics.mean(naive_quality_scores),
                "graphrag_median": statistics.median(graph_quality_scores),
                "naive_rag_median": statistics.median(naive_quality_scores)
            },
            "category_breakdown": category_stats
        }

        return summary

    def save_comparison_results(self, comparison_data: Dict[str, Any]) -> Path:
        """Save comparison results to file."""
        output_file = self.results_dir / "system_comparison_results.json"

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(comparison_data, f, indent=2, ensure_ascii=False)

        logger.info(f"‚úì Comparison results saved to: {output_file}")
        return output_file

    def generate_comparison_table(self, comparison_data: Dict[str, Any]) -> str:
        """Generate a readable comparison table."""
        results = comparison_data["results"]
        summary = comparison_data["summary"]

        table_lines = []
        table_lines.append("# GraphRAG vs Naive RAG Comparison Results")
        table_lines.append("")
        table_lines.append("## Summary")
        table_lines.append(f"- **GraphRAG Wins**: {summary['overall_performance']['graphrag_wins']}")
        table_lines.append(f"- **Naive RAG Wins**: {summary['overall_performance']['naive_rag_wins']}")
        table_lines.append(f"- **Ties**: {summary['overall_performance']['ties']}")
        table_lines.append(f"- **GraphRAG Win Rate**: {summary['overall_performance']['graphrag_win_rate']:.1%}")
        table_lines.append("")
        table_lines.append(f"- **GraphRAG Avg Quality**: {summary['quality_scores']['graphrag_avg']:.2f}")
        table_lines.append(f"- **Naive RAG Avg Quality**: {summary['quality_scores']['naive_rag_avg']:.2f}")
        table_lines.append("")

        # Category breakdown
        table_lines.append("## Performance by Category")
        table_lines.append("")
        for category, stats in summary["category_breakdown"].items():
            win_rate = stats["graph_wins"] / stats["total"]
            table_lines.append(f"### {category.title()}")
            table_lines.append(f"- GraphRAG: {stats['graph_wins']}/{stats['total']} ({win_rate:.1%})")
            table_lines.append("")

        # Detailed results table
        table_lines.append("## Detailed Results")
        table_lines.append("")
        table_lines.append("| # | Question | Category | GraphRAG Answer | Naive RAG Answer | Ground Truth | Winner |")
        table_lines.append("|---|----------|----------|-----------------|------------------|--------------|--------|")

        for result in results:
            idx = result["question_index"]
            question = result["question"][:50] + "..." if len(result["question"]) > 50 else result["question"]
            category = result["category"]

            graph_answer = result["graphrag"]["answer"][:30] + "..." if len(result["graphrag"]["answer"]) > 30 else result["graphrag"]["answer"]
            naive_answer = result["naive_rag"]["answer"][:30] + "..." if len(result["naive_rag"]["answer"]) > 30 else result["naive_rag"]["answer"]
            ground_truth = result["ground_truth"][:30] + "..." if len(result["ground_truth"]) > 30 else result["ground_truth"]

            graph_score = result["graphrag"]["evaluation"]["quality_score"]
            naive_score = result["naive_rag"]["evaluation"]["quality_score"]

            if graph_score > naive_score:
                winner = "GraphRAG ‚úÖ"
            elif naive_score > graph_score:
                winner = "Naive RAG ‚úÖ"
            else:
                winner = "Tie ‚öñÔ∏è"

            # Escape pipes in text
            question = question.replace("|", "\\|")
            graph_answer = graph_answer.replace("|", "\\|")
            naive_answer = naive_answer.replace("|", "\\|")
            ground_truth = ground_truth.replace("|", "\\|")

            table_lines.append(f"| {idx} | {question} | {category} | {graph_answer} | {naive_answer} | {ground_truth} | {winner} |")

        return "\n".join(table_lines)

    def display_results(self, comparison_data: Dict[str, Any]) -> None:
        """Display comparison results."""
        summary = comparison_data["summary"]

        print("\n" + "="*80)
        print("GRAPHRAG vs NAIVE RAG COMPARISON RESULTS")
        print("="*80)

        print(f"\nüìä OVERALL PERFORMANCE:")
        print(f"GraphRAG Wins: {summary['overall_performance']['graphrag_wins']}")
        print(f"Naive RAG Wins: {summary['overall_performance']['naive_rag_wins']}")
        print(f"Ties: {summary['overall_performance']['ties']}")
        print(f"GraphRAG Win Rate: {summary['overall_performance']['graphrag_win_rate']:.1%}")

        print(f"\nüéØ QUALITY SCORES:")
        print(f"GraphRAG Average: {summary['quality_scores']['graphrag_avg']:.2f}")
        print(f"Naive RAG Average: {summary['quality_scores']['naive_rag_avg']:.2f}")

        print(f"\nüìã PERFORMANCE BY CATEGORY:")
        for category, stats in summary["category_breakdown"].items():
            win_rate = stats["graph_wins"] / stats["total"]
            print(f"{category.title()}: GraphRAG {stats['graph_wins']}/{stats['total']} ({win_rate:.1%})")


async def main():
    """Main comparison function with enhanced output."""
    print("üöÄ GraphRAG vs Naive RAG Complete Comparison Workflow")
    print("=" * 60)

    try:
        # Initialize comparator first to load config
        comparator = SystemComparator()

        # Check if CV data exists using config path
        data_dir = Path(comparator.config.get('output', {}).get('programmers_dir', 'data/programmers'))
        if not data_dir.exists() or not list(data_dir.glob("*.pdf")):
            print(f"\n‚ö†Ô∏è  No CV PDFs found in {data_dir}")
            print("Please run: python 1_generate_data.py")
            print("Then run: python 2_data_to_knowledge_graph.py")
            return False

        print("\n‚úÖ CV data found successfully!")

        # Step 1: Ground Truth (handled automatically by SystemComparator)
        print("\n" + "="*60)
        print("STEP 1: Load/Generate Ground Truth using GPT-5")
        print("="*60)

        # Step 2: System Initialization and Comparison
        print("\n" + "="*60)
        print("STEP 2: Run Complete System Comparison")
        print("="*60)

        # Run full comparison
        print("\nStarting comprehensive comparison...")
        comparison_data = await comparator.run_full_comparison()

        # Save results
        output_file = comparator.save_comparison_results(comparison_data)

        # Generate markdown table
        markdown_table = comparator.generate_comparison_table(comparison_data)
        table_file = Path("results") / "comparison_table.md"
        with open(table_file, 'w') as f:
            f.write(markdown_table)

        # Display results
        comparator.display_results(comparison_data)

        # Final results summary
        print("\n" + "="*60)
        print("üéâ COMPARISON WORKFLOW COMPLETED SUCCESSFULLY!")
        print("="*60)

        print("\nüìÅ Generated Files:")
        results_dir = Path("results")
        if results_dir.exists():
            for file in sorted(results_dir.glob("*")):
                print(f"  ‚Ä¢ {file}")

        print("\nüìä Key Results:")
        print(f"  ‚Ä¢ Ground Truth: {results_dir}/ground_truth_answers.json")
        print(f"  ‚Ä¢ Comparison Data: {output_file}")
        print(f"  ‚Ä¢ Comparison Table: {table_file}")

        print("\nüîó Next Steps:")
        print(f"  1. Review the comparison table: cat {table_file}")
        print(f"  2. Analyze detailed results: cat {output_file}")

        return True

    except Exception as e:
        logger.error(f"Comparison failed: {e}")
        print(f"\nüí• Workflow failed: {e}")
        print("Check the errors above for details.")
        return False


if __name__ == "__main__":
    asyncio.run(main())