#!/usr/bin/env python3
"""
GraphRAG vs Naive RAG Comparison System
======================================

Compares GraphRAG and Naive RAG performance using GPT-5 ground truth
to demonstrate the advantages of graph-based retrieval for structured queries.
"""

from dotenv import load_dotenv
load_dotenv(override=True)

import json
import time
import asyncio
import importlib.util
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
import logging
import toml
import os
import sys
import subprocess
import httpx
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from ragas import evaluate, EvaluationDataset
from ragas.metrics import ContextPrecision, Faithfulness, AnswerRelevancy, ContextRecall
from ragas.dataset_schema import SingleTurnSample
import asyncio

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SystemComparator:
    """Compares GraphRAG, Naive RAG, and Ground Truth systems."""

    def __init__(self, config_path: str = "utils/config.toml"):
        """Initialize the comparator."""
        self.config = self._load_config(config_path)
        self.results_dir = Path("results")
        self.results_dir.mkdir(exist_ok=True)

        # Backend URL (env overrides config)
        self.backend_url = os.getenv(
            "BACKEND_URL",
            self.config.get("backend", {}).get("url", "http://localhost:8000")
        ).rstrip("/")

        # Systems will be initialized when needed
        self.graph_rag_system = None
        self.naive_rag_system = None

        self.api_key = os.environ.get("OPENAI_API_KEY")

        if not self.api_key:
            raise ValueError("Missing OPENAI_API_KEY!")
        
        # 3. Inicjalizacja modeli dla Ragasa
        logger.info("Initializing OpenAI models for evaluation...")
        
        self.eval_llm = ChatOpenAI(
            model="gpt-4o",  # or gpt-4-turbo
            temperature=0,
            api_key=self.api_key
        )
        
        self.eval_embeddings = OpenAIEmbeddings(
            model="text-embedding-3-small",
            api_key=self.api_key
        )

        logger.info(f"âœ“ System Comparator initialized (backend={self.backend_url})")

    def _find_script(self, names: List[str]) -> Optional[Path]:
        """Search repository for scripts by filename (returns first match)."""
        repo_root = Path(__file__).resolve().parents[2]
        # try direct lookups first
        for name in names:
            p = repo_root / name
            if p.exists():
                logger.info(f"Found script by direct path: {p}")
                return p

        # fallback: search the whole repo
        for name in names:
            matches = list(repo_root.rglob(name))
            if matches:
                logger.info(f"Found script by rglob: {matches[0]}")
                return matches[0]

        # last resort: search cwd
        for name in names:
            matches = list(Path.cwd().rglob(name))
            if matches:
                logger.info(f"Found script in cwd by rglob: {matches[0]}")
                return matches[0]

        logger.warning(f"Could not find any of {names}")
        return None
    
    def _load_config(self, config_path: str) -> dict:
        """Load configuration from TOML file."""
        if not os.path.exists(config_path):
            raise ValueError(f"Configuration file not found: {config_path}")

        with open(config_path, 'r') as f:
            config = toml.load(f)

        return config

    def load_ground_truth(self) -> Dict[str, Any]:
        """Load ground truth answers generated by GPT-5."""
        ground_truth_file = self.results_dir / "ground_truth_answers.json"

        logger.info(f"âœ“ Using existing ground truth: {ground_truth_file}")

        with open(ground_truth_file, 'r') as f:
            data = json.load(f)

        logger.info(f"âœ“ Loaded ground truth with {len(data['ground_truth_answers'])} questions")
        return data

    def initialize_graph_rag_system(self):
        """Check backend /ask_graph endpoint is available."""
        url = f"{self.backend_url}/ask_graph"
        try:
            resp = httpx.post(url, json={"question": "Health check"}, timeout=10.0)
            resp.raise_for_status()
            self.graph_rag_system = True
            logger.info("âœ“ GraphRAG backend endpoint available")
            return True
        except Exception as e:
            logger.error(f"GraphRAG backend endpoint not reachable ({url}): {e}")
            return False

    def initialize_naive_rag_system(self):
        """Check backend /ask_rag endpoint is available."""
        url = f"{self.backend_url}/ask_rag"
        try:
            resp = httpx.post(url, json={"question": "Health check", "top_k": 1}, timeout=10.0)
            resp.raise_for_status()
            self.naive_rag_system = True
            logger.info("âœ“ Naive RAG backend endpoint available")
            return True
        except Exception as e:
            logger.error(f"Naive RAG backend endpoint not reachable ({url}): {e}")
            return False

        except Exception as e:
            logger.error(f"Failed to initialize Naive RAG system: {e}")
            return False

    def run_graph_rag_query(self, question: str) -> Dict[str, Any]:
        """Run a query through the GraphRAG backend (/ask_graph)."""
        try:
            start_time = time.time()
            url = f"{self.backend_url}/ask_graph"
            resp = httpx.post(url, json={"question": question}, timeout=30.0)
            resp.raise_for_status()
            execution_time = time.time() - start_time

            data = resp.json()
            return {
                "question": question,
                "answer": data.get("answer", "No answer"),
                "cypher_query": data.get("cypher_query", ""),
                "execution_time": execution_time,
                "success": True,
                "system": "graphrag",
                "raw": data
            }

        except Exception as e:
            logger.error(f"GraphRAG query failed for '{question}': {e}")
            return {
                "question": question,
                "answer": f"Error: {str(e)}",
                "cypher_query": "",
                "execution_time": 0,
                "success": False,
                "system": "graphrag",
                "error": str(e)
            }

    def run_naive_rag_query(self, question: str) -> Dict[str, Any]:
        """Run a query through the Naive RAG backend (/ask_rag)."""
        try:
            start_time = time.time()
            url = f"{self.backend_url}/ask_rag"
            top_k = int(self.config.get("evaluation", {}).get("top_k", 5))
            resp = httpx.post(url, json={"question": question, "top_k": top_k}, timeout=30.0)
            resp.raise_for_status()
            execution_time = time.time() - start_time

            data = resp.json()
            return {
                "question": question,
                "answer": data.get("answer", "No answer"),
                "execution_time": execution_time,
                "num_chunks_retrieved": len(data.get("context_documents", [])),
                "success": True,
                "system": "naive_rag",
                "raw": data
            }

        except Exception as e:
            logger.error(f"Naive RAG query failed for '{question}': {e}")
            return {
                "question": question,
                "answer": f"Error: {str(e)}",
                "execution_time": 0,
                "num_chunks_retrieved": 0,
                "success": False,
                "system": "naive_rag",
                "error": str(e)
            }

    async def run_full_comparison(self) -> Dict[str, Any]:
        """Run complete comparison utilizing Ragas for evaluation."""
        
        # Data preparation
        ground_truth_data = self.load_ground_truth()
        questions = ground_truth_data["ground_truth_answers"]

        if not self.initialize_graph_rag_system() or not self.initialize_naive_rag_system():
            raise Exception("Failed to initialize RAG systems")

        samples_graph = []
        samples_naive = []
        # Container for operational metadata (execution time, success, cypher query etc.)
        operational_data = []

        logger.info(f"Starting generation for {len(questions)} questions...")

        # Generating answers
        for i, item in enumerate(questions):
            q = item["question"]
            gt = item["ground_truth_answer"]
            
            logger.info(f"[{i+1}/{len(questions)}] Generating: {q[:40]}...")

            graph_res = self.run_graph_rag_query(q)
            # Convert context to strings
            graph_context_str = [str(c) for c in graph_res.get("context", [])] 
            
            samples_graph.append(SingleTurnSample(
                user_input=q,
                response=graph_res["answer"],
                retrieved_contexts=graph_context_str,
                reference=gt
            ))

            naive_res = self.run_naive_rag_query(q)
            naive_context_str = naive_res.get("retrieved_contexts", [])

            samples_naive.append(SingleTurnSample(
                user_input=q,
                response=naive_res["answer"],
                retrieved_contexts=naive_context_str,
                reference=gt
            ))

            operational_data.append({
                "question_index": i + 1,
                "question": q,
                "category": item["category"],
                "ground_truth": gt,
                "graphrag_meta": {
                    "execution_time": graph_res["execution_time"],
                    "cypher_query": graph_res.get("cypher_query", ""),
                    "success": graph_res["success"]
                },
                "naiverag_meta": {
                    "execution_time": naive_res["execution_time"],
                    "chunks_count": naive_res.get("num_chunks_retrieved", 0),
                    "success": naive_res["success"]
                }
            })
            
            # Pause for rate limits
            await asyncio.sleep(0.5)

        # Evaluation
        logger.info("Running Ragas evaluation (this may take a while)...")
        
        metrics = [ContextPrecision(), Faithfulness(), AnswerRelevancy(), ContextRecall()]

        logger.info("Evaluating GraphRAG results...")
        graph_scores = evaluate(
            dataset=EvaluationDataset(samples_graph), 
            metrics=metrics,
            llm=self.eval_llm,
            embeddings=self.eval_embeddings
        )
        
        logger.info("Evaluating Naive RAG results...")
        naive_scores = evaluate(
            dataset=EvaluationDataset(samples_naive), 
            metrics=metrics,
            llm=self.eval_llm,
            embeddings=self.eval_embeddings
        )

        # Merging results
        graph_scores_list = graph_scores.to_pandas().to_dict(orient="records")
        naive_scores_list = naive_scores.to_pandas().to_dict(orient="records")

        final_results = []
        
        for i, op_data in enumerate(operational_data):
            
            # WyciÄ…gamy wyniki dla konkretnego pytania
            # Use the pandas-converted lists (indexable by integer)
            g_score = graph_scores_list[i]
            n_score = naive_scores_list[i]

            comparison_entry = {
                **op_data,
                "graphrag": {
                    "answer": samples_graph[i].response,
                    **op_data["graphrag_meta"],
                    "evaluation": g_score
                },
                "naive_rag": {
                    "answer": samples_naive[i].response,
                    **op_data["naiverag_meta"],
                    "evaluation": n_score
                }
            }
            # Deleting temporary meta keys, to avoid duplication
            del comparison_entry["graphrag_meta"]
            del comparison_entry["naiverag_meta"]
            
            final_results.append(comparison_entry)

        # Build a proper summary using `generate_summary` and include ragas numeric averages
        ragas_avg = {
            "graphrag_avg": graph_scores.to_pandas().mean(numeric_only=True).to_dict(),
            "naive_avg": naive_scores.to_pandas().mean(numeric_only=True).to_dict()
        }

        summary = self.generate_summary(final_results)
        summary["ragas_averages"] = ragas_avg

        return {
            "metadata": {
                "total_questions": len(questions),
                "evaluator": "Ragas + OpenAI",
                "systems": ["GraphRAG", "Naive RAG"]
            },
            "results": final_results,
            "summary": summary
        }

    def _calculate_score(self, evaluation_metrics: Dict[str, float]) -> float:
        """Helper: Calculate average score from Ragas metrics."""
        if not evaluation_metrics:
            return 0.0
        # Average from available metrics (faithfulness, answer_relevancy, etc.)
        values = [v for v in evaluation_metrics.values() if isinstance(v, (int, float))]
        return sum(values) / len(values) if values else 0.0

    def generate_summary(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Generate summary statistics for the comparison (Adapted for Ragas)."""
        graph_wins = 0
        naive_wins = 0
        ties = 0

        graph_quality_scores = []
        naive_quality_scores = []

        category_stats = {}

        for result in results:
            category = result["category"]
            
            graph_metrics = result["graphrag"]["evaluation"]
            naive_metrics = result["naive_rag"]["evaluation"]
            
            graph_score = self._calculate_score(graph_metrics)
            naive_score = self._calculate_score(naive_metrics)

            graph_quality_scores.append(graph_score)
            naive_quality_scores.append(naive_score)

            # Track category performance
            if category not in category_stats:
                category_stats[category] = {
                    "total": 0,
                    "graph_wins": 0,
                    "naive_wins": 0,
                    "ties": 0
                }

            category_stats[category]["total"] += 1

            # Determine winner (with small tolerance for floats)
            if graph_score > naive_score + 0.01:
                graph_wins += 1
                category_stats[category]["graph_wins"] += 1
            elif naive_score > graph_score + 0.01:
                naive_wins += 1
                category_stats[category]["naive_wins"] += 1
            else:
                ties += 1
                category_stats[category]["ties"] += 1

        import statistics

        summary = {
            "overall_performance": {
                "graphrag_wins": graph_wins,
                "naive_rag_wins": naive_wins,
                "ties": ties,
                "graphrag_win_rate": graph_wins / len(results) if results else 0,
                "naive_rag_win_rate": naive_wins / len(results) if results else 0
            },
            "quality_scores": {
                "graphrag_avg": statistics.mean(graph_quality_scores) if graph_quality_scores else 0,
                "naive_rag_avg": statistics.mean(naive_quality_scores) if naive_quality_scores else 0,
                "graphrag_median": statistics.median(graph_quality_scores) if graph_quality_scores else 0,
                "naive_rag_median": statistics.median(naive_quality_scores) if naive_quality_scores else 0
            },
            "category_breakdown": category_stats
        }

        return summary

    def save_comparison_results(self, comparison_data: Dict[str, Any]) -> Path:
        """Save comparison results to file."""
        output_file = self.results_dir / "system_comparison_results.json"

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(comparison_data, f, indent=2, ensure_ascii=False)

        logger.info(f"âœ“ Comparison results saved to: {output_file}")
        return output_file

    def generate_comparison_table(self, comparison_data: Dict[str, Any]) -> str:
        """Generate a readable comparison table."""
        results = comparison_data["results"]
        summary = comparison_data["summary"]

        table_lines = []
        table_lines.append("# GraphRAG vs Naive RAG Comparison Results")
        table_lines.append("")
        table_lines.append("## Summary")
        table_lines.append(f"- **GraphRAG Wins**: {summary['overall_performance']['graphrag_wins']}")
        table_lines.append(f"- **Naive RAG Wins**: {summary['overall_performance']['naive_rag_wins']}")
        table_lines.append(f"- **Ties**: {summary['overall_performance']['ties']}")
        table_lines.append(f"- **GraphRAG Win Rate**: {summary['overall_performance']['graphrag_win_rate']:.1%}")
        table_lines.append("")
        table_lines.append(f"- **GraphRAG Avg Quality**: {summary['quality_scores']['graphrag_avg']:.2f}")
        table_lines.append(f"- **Naive RAG Avg Quality**: {summary['quality_scores']['naive_rag_avg']:.2f}")
        table_lines.append("")

        # Category breakdown
        table_lines.append("## Performance by Category")
        table_lines.append("")
        for category, stats in summary["category_breakdown"].items():
            total = stats["total"]
            if total > 0:
                win_rate = stats["graph_wins"] / total
                table_lines.append(f"### {category.title()}")
                table_lines.append(f"- GraphRAG: {stats['graph_wins']}/{total} ({win_rate:.1%})")
                table_lines.append("")

        # Detailed results table
        table_lines.append("## Detailed Results")
        table_lines.append("")
        # Updated header to show scores
        table_lines.append("| # | Question | Graph Answer | Naive Answer | Graph Score | Naive Score | Winner |")
        table_lines.append("|---|----------|--------------|--------------|-------------|-------------|--------|")

        for result in results:
            idx = result["question_index"]
            question = result["question"][:40] + "..." if len(result["question"]) > 40 else result["question"]
            
            graph_answer = result["graphrag"]["answer"][:30] + "..." if len(result["graphrag"]["answer"]) > 30 else result["graphrag"]["answer"]
            naive_answer = result["naive_rag"]["answer"][:30] + "..." if len(result["naive_rag"]["answer"]) > 30 else result["naive_rag"]["answer"]

            graph_score = self._calculate_score(result["graphrag"]["evaluation"])
            naive_score = self._calculate_score(result["naive_rag"]["evaluation"])

            if graph_score > naive_score + 0.01:
                winner = "GraphRAG"
            elif naive_score > graph_score + 0.01:
                winner = "Naive RAG"
            else:
                winner = "Tie"

            # Escape pipes
            question = question.replace("|", "\\|").replace("\n", " ")
            graph_answer = graph_answer.replace("|", "\\|").replace("\n", " ")
            naive_answer = naive_answer.replace("|", "\\|").replace("\n", " ")

            table_lines.append(f"| {idx} | {question} | {graph_answer} | {naive_answer} | {graph_score:.2f} | {naive_score:.2f} | {winner} |")

        return "\n".join(table_lines)
    
    def display_results(self, comparison_data: Dict[str, Any]) -> None:
        """Display comparison results to stdout."""
        summary = comparison_data["summary"]
        perf = summary["overall_performance"]
        scores = summary["quality_scores"]

        print("\n" + "="*80)
        print("GRAPHRAG vs NAIVE RAG COMPARISON RESULTS")
        print("="*80)

        print(f"\nOVERALL WINNER:")
        if perf["graphrag_wins"] > perf["naive_rag_wins"]:
            print(f"   GraphRAG wins! ({perf['graphrag_wins']} vs {perf['naive_rag_wins']})")
        elif perf["naive_rag_wins"] > perf["graphrag_wins"]:
            print(f"   Naive RAG wins! ({perf['naive_rag_wins']} vs {perf['graphrag_wins']})")
        else:
            print(f"   It's a tie! ({perf['graphrag_wins']} vs {perf['naive_rag_wins']})")

        print(f"\nWIN RATES:")
        print(f"   â€¢ GraphRAG Win Rate: {perf['graphrag_win_rate']:.1%}")
        print(f"   â€¢ Naive RAG Win Rate: {perf['naive_rag_win_rate']:.1%}")
        print(f"   â€¢ Ties: {perf['ties']}")

        print(f"\nAVERAGE QUALITY SCORES:")
        print(f"   â€¢ GraphRAG:  {scores['graphrag_avg']:.4f}")
        print(f"   â€¢ Naive RAG: {scores['naive_rag_avg']:.4f}")

        print(f"\nBREAKDOWN BY CATEGORY:")
        for category, stats in summary["category_breakdown"].items():
            total = stats["total"]
            if total > 0:
                g_rate = stats["graph_wins"] / total
                n_rate = stats["naive_wins"] / total
                print(f"   â€¢ {category.title():<15}: GraphRAG {stats['graph_wins']} wins ({g_rate:.0%}) | Naive {stats['naive_wins']} wins ({n_rate:.0%})")
        
        print("\n" + "="*80)

async def main():
    """Main comparison function with enhanced output."""
    print("ðŸš€ GraphRAG vs Naive RAG Complete Comparison Workflow")
    print("=" * 60)

    try:
        # Initialize comparator first to load config
        comparator = SystemComparator()

        # Check if CV data exists using config path
        data_dir = Path(comparator.config.get('output', {}).get('programmers_dir', 'data/programmers'))
        if not data_dir.exists() or not list(data_dir.glob("*.pdf")):
            print(f"\nNo CV PDFs found in {data_dir}")
            print("Please run: python 1_generate_data.py")
            print("Then run: python 2_data_to_knowledge_graph.py")
            return False

        print("\nCV data found successfully!")

        # Step 1: Ground Truth (handled automatically by SystemComparator)
        print("\n" + "="*60)
        print("STEP 1: Load/Generate Ground Truth using GPT-5")
        print("="*60)

        # Step 2: System Initialization and Comparison
        print("\n" + "="*60)
        print("STEP 2: Run Complete System Comparison")
        print("="*60)

        # Run full comparison
        print("\nStarting comprehensive comparison...")
        comparison_data = await comparator.run_full_comparison()

        # Save results
        output_file = comparator.save_comparison_results(comparison_data)

        # Generate markdown table
        markdown_table = comparator.generate_comparison_table(comparison_data)
        table_file = Path("results") / "comparison_table.md"
        with open(table_file, 'w') as f:
            f.write(markdown_table)

        # Display results
        comparator.display_results(comparison_data)

        # Final results summary
        print("\n" + "="*60)
        print("COMPARISON WORKFLOW COMPLETED SUCCESSFULLY!")
        print("="*60)
        print("\nGenerated Files:")
        results_dir = Path("results")
        if results_dir.exists():
            for file in sorted(results_dir.glob("*")):
                print(f"  â€¢ {file}")

        print("\nKey Results:")
        print(f"  â€¢ Ground Truth: {results_dir}/ground_truth_answers.json")
        print(f"  â€¢ Comparison Data: {output_file}")
        print(f"  â€¢ Comparison Table: {table_file}")

        print("\nNext Steps:")
        print(f"  1. Review the comparison table: cat {table_file}")
        print(f"  2. Analyze detailed results: cat {output_file}")

        return True

    except Exception as e:
        logger.exception("Comparison failed")
        print("\nðŸ’¥ Workflow failed: see logs (traceback printed to logger)")
        print("Check the logs above for traceback/details.")
        return False


if __name__ == "__main__":
    asyncio.run(main())